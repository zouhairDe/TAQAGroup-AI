# Anomaly Criticality Prediction Model - Google Colab Training
# This notebook trains a model to predict equipment anomaly criticality (1-11 scale)
# and provides options to save/download the trained model for use in web applications

# ============================================================================
# INSTALLATION AND IMPORTS
# ============================================================================

# Install required packages
!pip install tensorflow pandas scikit-learn matplotlib seaborn

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Concatenate
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import files
import pickle
import json
import zipfile
import os

print("TensorFlow version:", tf.__version__)
print("Setup complete!")

# ============================================================================
# DATA UPLOAD AND LOADING
# ============================================================================

print("Please upload your CSV file containing the anomaly data:")
uploaded = files.upload()

# Get the uploaded file name
csv_filename = list(uploaded.keys())[0]
print(f"Uploaded file: {csv_filename}")

# Load the data
df = pd.read_csv(csv_filename)
print(f"Data shape: {df.shape}")
print("\nFirst few rows:")
print(df.head())

print("\nColumn names:")
print(df.columns.tolist())

print("\nData info:")
print(df.info())

# ============================================================================
# DATA EXPLORATION AND PREPROCESSING
# ============================================================================

# Check for missing values
print("\nMissing values:")
print(df.isnull().sum())

# Display criticality distribution
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
df['Criticité'].value_counts().sort_index().plot(kind='bar')
plt.title('Distribution of Criticality Levels')
plt.xlabel('Criticality')
plt.ylabel('Count')

plt.subplot(1, 2, 2)
plt.hist(df['Criticité'], bins=20, edgecolor='black', alpha=0.7)
plt.title('Histogram of Criticality Levels')
plt.xlabel('Criticality')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

# Basic statistics
print("\nCriticality Statistics:")
print(df['Criticité'].describe())

# Check unique values in categorical columns
categorical_cols = ['Systeme', 'Description de l\'équipement', 'Section propriétaire']
for col in categorical_cols:
    if col in df.columns:
        print(f"\n{col} - Unique values: {df[col].nunique()}")
        if df[col].nunique() < 20:
            print(df[col].value_counts().head(10))

# ============================================================================
# DATA CLEANING AND PREPROCESSING
# ============================================================================

# Clean the data
def clean_data(df):
    # Remove rows with missing critical information
    required_cols = ['Description', 'Systeme', 'Description de l\'équipement', 
                    'Section propriétaire', 'Fiabilité Intégrité', 'Process Safety', 'Criticité']
    
    cleaned_df = df.dropna(subset=required_cols)
    
    # Clean text data
    cleaned_df['Description'] = cleaned_df['Description'].astype(str).str.lower()
    
    print(f"Original data: {len(df)} rows")
    print(f"After cleaning: {len(cleaned_df)} rows")
    
    return cleaned_df

# Clean the data
df_clean = clean_data(df)

# Prepare features
def prepare_features(df):
    # Text preprocessing
    tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')
    tokenizer.fit_on_texts(df['Description'])
    
    # Convert text to sequences
    sequences = tokenizer.texts_to_sequences(df['Description'])
    max_length = min(100, max([len(seq) for seq in sequences]))  # Cap at 100 words
    X_text = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')
    
    # Encode categorical features
    label_encoders = {}
    categorical_features = []
    
    for col in ['Systeme', 'Description de l\'équipement', 'Section propriétaire']:
        if col in df.columns:
            le = LabelEncoder()
            encoded = le.fit_transform(df[col].astype(str))
            categorical_features.append(encoded)
            label_encoders[col] = le
    
    # Combine categorical features
    if categorical_features:
        X_categorical = np.column_stack(categorical_features)
    else:
        X_categorical = np.zeros((len(df), 1))
    
    # Numerical features
    numerical_cols = ['Fiabilité Intégrité', 'Process Safety']
    X_numerical = df[numerical_cols].values
    
    # Target variable
    y = df['Criticité'].values
    
    return X_text, X_categorical, X_numerical, y, tokenizer, label_encoders, max_length

# Prepare features
X_text, X_categorical, X_numerical, y, tokenizer, label_encoders, max_length = prepare_features(df_clean)

print(f"Text features shape: {X_text.shape}")
print(f"Categorical features shape: {X_categorical.shape}")
print(f"Numerical features shape: {X_numerical.shape}")
print(f"Target shape: {y.shape}")
print(f"Max sequence length: {max_length}")

# ============================================================================
# MODEL ARCHITECTURE
# ============================================================================

def create_model(vocab_size, max_length, categorical_dim, numerical_dim):
    # Text input branch
    text_input = Input(shape=(max_length,), name='text_input')
    embedding = Embedding(vocab_size, 128, input_length=max_length, mask_zero=True)(text_input)
    lstm1 = LSTM(64, return_sequences=True, dropout=0.3)(embedding)
    lstm2 = LSTM(32, dropout=0.3)(lstm1)
    
    # Categorical input branch
    categorical_input = Input(shape=(categorical_dim,), name='categorical_input')
    categorical_dense = Dense(16, activation='relu')(categorical_input)
    
    # Numerical input branch
    numerical_input = Input(shape=(numerical_dim,), name='numerical_input')
    numerical_dense = Dense(8, activation='relu')(numerical_input)
    
    # Combine all branches
    combined = Concatenate()([lstm2, categorical_dense, numerical_dense])
    
    # Final layers
    dense1 = Dense(64, activation='relu')(combined)
    dropout1 = Dropout(0.4)(dense1)
    dense2 = Dense(32, activation='relu')(dropout1)
    dropout2 = Dropout(0.3)(dense2)
    dense3 = Dense(16, activation='relu')(dropout2)
    
    # Output layer (regression for criticality score 1-11)
    output = Dense(1, activation='linear', name='criticality_output')(dense3)
    
    model = Model(inputs=[text_input, categorical_input, numerical_input], outputs=output)
    
    return model

# Create the model
vocab_size = min(5000, len(tokenizer.word_index)) + 1
model = create_model(vocab_size, max_length, X_categorical.shape[1], X_numerical.shape[1])

# Compile the model
model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='huber',  # Robust to outliers
    metrics=['mae', 'mse']
)

# Display model architecture
model.summary()

# Visualize model architecture
tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True, dpi=150)

# ============================================================================
# MODEL TRAINING
# ============================================================================

# Split the data
X_text_train, X_text_test, X_cat_train, X_cat_test, X_num_train, X_num_test, y_train, y_test = train_test_split(
    X_text, X_categorical, X_numerical, y, test_size=0.2, random_state=42, stratify=None
)

print(f"Training set size: {len(X_text_train)}")
print(f"Test set size: {len(X_text_test)}")

# Define callbacks
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True,
    verbose=1
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=5,
    min_lr=1e-7,
    verbose=1
)

# Train the model
print("Starting model training...")
history = model.fit(
    [X_text_train, X_cat_train, X_num_train],
    y_train,
    batch_size=32,
    epochs=100,
    validation_data=([X_text_test, X_cat_test, X_num_test], y_test),
    callbacks=[early_stopping, reduce_lr],
    verbose=1
)

print("Training completed!")

# ============================================================================
# MODEL EVALUATION
# ============================================================================

# Evaluate the model
train_loss, train_mae, train_mse = model.evaluate([X_text_train, X_cat_train, X_num_train], y_train, verbose=0)
test_loss, test_mae, test_mse = model.evaluate([X_text_test, X_cat_test, X_num_test], y_test, verbose=0)

print(f"\nTraining Results:")
print(f"  Loss: {train_loss:.4f}")
print(f"  MAE: {train_mae:.4f}")
print(f"  RMSE: {np.sqrt(train_mse):.4f}")

print(f"\nTest Results:")
print(f"  Loss: {test_loss:.4f}")
print(f"  MAE: {test_mae:.4f}")
print(f"  RMSE: {np.sqrt(test_mse):.4f}")

# Make predictions
y_pred = model.predict([X_text_test, X_cat_test, X_num_test])
y_pred_flat = y_pred.flatten()

# Plot training history
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Loss
axes[0, 0].plot(history.history['loss'], label='Training Loss')
axes[0, 0].plot(history.history['val_loss'], label='Validation Loss')
axes[0, 0].set_title('Model Loss')
axes[0, 0].set_xlabel('Epoch')
axes[0, 0].set_ylabel('Loss')
axes[0, 0].legend()

# MAE
axes[0, 1].plot(history.history['mae'], label='Training MAE')
axes[0, 1].plot(history.history['val_mae'], label='Validation MAE')
axes[0, 1].set_title('Model MAE')
axes[0, 1].set_xlabel('Epoch')
axes[0, 1].set_ylabel('MAE')
axes[0, 1].legend()

# Predictions vs Actual
axes[1, 0].scatter(y_test, y_pred_flat, alpha=0.6)
axes[1, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
axes[1, 0].set_xlabel('Actual Criticality')
axes[1, 0].set_ylabel('Predicted Criticality')
axes[1, 0].set_title('Predictions vs Actual')

# Residuals
residuals = y_test - y_pred_flat
axes[1, 1].scatter(y_pred_flat, residuals, alpha=0.6)
axes[1, 1].axhline(y=0, color='r', linestyle='--')
axes[1, 1].set_xlabel('Predicted Criticality')
axes[1, 1].set_ylabel('Residuals')
axes[1, 1].set_title('Residual Plot')

plt.tight_layout()
plt.show()

# Classification report (treating as classification problem)
from sklearn.metrics import classification_report, confusion_matrix

# Round predictions to nearest integer for classification analysis
y_test_rounded = np.round(y_test).astype(int)
y_pred_rounded = np.round(y_pred_flat).astype(int)

print("\nClassification Report (rounded predictions):")
print(classification_report(y_test_rounded, y_pred_rounded))

# Confusion matrix
plt.figure(figsize=(10, 8))
cm = confusion_matrix(y_test_rounded, y_pred_rounded)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Criticality')
plt.ylabel('Actual Criticality')
plt.show()

# ============================================================================
# MODEL SAVING AND EXPORT
# ============================================================================

# Save the trained model
model_filename = 'anomaly_criticality_model.h5'
model.save(model_filename)
print(f"Model saved as {model_filename}")

# Save preprocessing objects
preprocessing_data = {
    'tokenizer': tokenizer,
    'label_encoders': label_encoders,
    'max_length': max_length,
    'vocab_size': vocab_size,
    'categorical_columns': ['Systeme', 'Description de l\'équipement', 'Section propriétaire'],
    'numerical_columns': ['Fiabilité Intégrité', 'Process Safety']
}

with open('preprocessing_data.pkl', 'wb') as f:
    pickle.dump(preprocessing_data, f)

print("Preprocessing data saved as preprocessing_data.pkl")

# --- Place this helper function near your web_config export ---
def to_py(obj):
    if isinstance(obj, (np.integer, np.int32, np.int64)):
        return int(obj)
    if isinstance(obj, (np.floating, np.float32, np.float64)):
        return float(obj)
    if isinstance(obj, dict):
        return {k: to_py(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [to_py(v) for v in obj]
    return obj

# --- Replace your web_config and json.dump block with this ---
web_config = {
    'model_architecture': {
        'vocab_size': int(vocab_size),
        'max_length': int(max_length),
        'categorical_dim': int(X_categorical.shape[1]),
        'numerical_dim': int(X_numerical.shape[1])
    },
    'tokenizer_config': {
        'word_index': {k: int(v) for k, v in tokenizer.word_index.items()},
        'num_words': int(tokenizer.num_words),
        'oov_token': tokenizer.oov_token
    },
    'label_encoders': {
        col: {
            'classes': le.classes_.tolist(),
            'mapping': {str(k): int(v) for k, v in zip(le.classes_, le.transform(le.classes_))}
        } for col, le in label_encoders.items()
    },
    'performance_metrics': {
        'test_mae': float(test_mae),
        'test_rmse': float(np.sqrt(test_mse)),
        'test_loss': float(test_loss)
    }
}
with open('web_config.json', 'w') as f:
    json.dump(to_py(web_config), f, indent=2)

print("Web configuration saved as web_config.json")

# ============================================================================
# TENSORFLOW.JS CONVERSION (for web deployment)
# ============================================================================

# Install tensorflowjs
!pip install tensorflowjs

import tensorflowjs as tfjs

# Convert model to TensorFlow.js format
tfjs_model_dir = 'tfjs_model'
tfjs.converters.save_keras_model(model, tfjs_model_dir)
print(f"TensorFlow.js model saved in {tfjs_model_dir} directory")

# ============================================================================
# CREATE DEPLOYMENT PACKAGE
# ============================================================================

# Create a zip file with all necessary files for deployment
def create_deployment_package():
    zip_filename = 'anomaly_model_deployment.zip'
    
    with zipfile.ZipFile(zip_filename, 'w') as zipf:
        # Add the saved model
        zipf.write(model_filename)
        zipf.write('preprocessing_data.pkl')
        zipf.write('web_config.json')
        
        # Add TensorFlow.js model files
        for root, dirs, files in os.walk(tfjs_model_dir):
            for file in files:
                file_path = os.path.join(root, file)
                arcname = os.path.relpath(file_path, '.')
                zipf.write(file_path, arcname)
    
    print(f"Deployment package created: {zip_filename}")
    return zip_filename

deployment_zip = create_deployment_package()

# ============================================================================
# DOWNLOAD FILES
# ============================================================================

print("\nDownloading model files...")

# Download individual files
files.download(model_filename)
files.download('preprocessing_data.pkl')
files.download('web_config.json')
files.download(deployment_zip)

print("All files downloaded successfully!")

# ============================================================================
# SAMPLE PREDICTION FUNCTION
# ============================================================================

def predict_criticality(description, systeme, equipement, section, fiabilite, safety):
    """
    Make a prediction for a new anomaly
    """
    # Preprocess text
    sequence = tokenizer.texts_to_sequences([description.lower()])
    text_input = pad_sequences(sequence, maxlen=max_length, padding='post', truncating='post')
    
    # Encode categorical features
    categorical_input = []
    for col in ['Systeme', 'Description de l\'équipement', 'Section propriétaire']:
        if col in label_encoders:
            value = eval(col.split(' ')[-1].lower())  # Get the variable value
            try:
                encoded = label_encoders[col].transform([str(value)])[0]
            except ValueError:
                encoded = 0  # Unknown category
            categorical_input.append(encoded)
    
    categorical_input = np.array([categorical_input])
    
    # Numerical features
    numerical_input = np.array([[fiabilite, safety]])
    
    # Make prediction
    prediction = model.predict([text_input, categorical_input, numerical_input])
    return float(prediction[0][0])

# Example prediction
example_description = "VIBRATION PALIER VENTILATEUR TIRAGE B COTE LIBRE"
example_systeme = "a738c0f1-e548-4680-9409-ef6832ec1e32"
example_equipement = "Ventilateur de tirage B"
example_section = "34MC"
example_fiabilite = 5
example_safety = 3

predicted_criticality = predict_criticality(
    example_description, example_systeme, example_equipement, 
    example_section, example_fiabilite, example_safety
)

print(f"\nExample Prediction:")
print(f"Description: {example_description}")
print(f"Predicted Criticality: {predicted_criticality:.2f}")
print(f"Rounded Criticality: {round(predicted_criticality)}")

print("\n" + "="*80)
print("TRAINING COMPLETE!")
print("="*80)
print("\nModel Performance Summary:")
print(f"• Test MAE: {test_mae:.3f}")
print(f"• Test RMSE: {np.sqrt(test_mse):.3f}")
print(f"• Model can predict criticality with average error of ±{test_mae:.1f} points")
print("\nFiles created:")
print("• anomaly_criticality_model.h5 - Trained Keras model")
print("• preprocessing_data.pkl - Tokenizer and encoders")
print("• web_config.json - Configuration for web deployment")
print("• tfjs_model/ - TensorFlow.js model for web deployment")
print("• anomaly_model_deployment.zip - Complete deployment package")
print("\nYou can now use these files in your web application!")